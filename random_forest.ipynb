{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Homework. Random Forest. \n",
    "\n",
    "Build a random forest based on dtree class. Use the same dataset cars for testing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class dtree:\n",
    "    \"\"\" A basic Decision Tree\"\"\"\n",
    "\n",
    "    def __init__(self):\n",
    "        \"\"\" Constructor \"\"\"\n",
    "\n",
    "    def read_data(self, filename):\n",
    "        fid = open(filename, \"r\")\n",
    "        data = []\n",
    "        d = []\n",
    "        for line in fid.readlines():\n",
    "            d.append(line.strip())\n",
    "        for d1 in d:\n",
    "            data.append(d1.split(\",\"))\n",
    "        fid.close()\n",
    "\n",
    "        self.featureNames = data[0]\n",
    "        self.featureNames = self.featureNames[:-1]\n",
    "        data = data[1:]\n",
    "        self.classes = []\n",
    "        for d in range(len(data)):\n",
    "            self.classes.append(data[d][-1])\n",
    "            data[d] = data[d][:-1]\n",
    "\n",
    "        return data, self.classes, self.featureNames\n",
    "\n",
    "    def classify(self, tree, datapoint):\n",
    "        \n",
    "        m = int(np.sqrt(len(self.featureNames)))\n",
    "        newFeatureNames = np.random.choice(self.featureNames, m, replace=False)\n",
    "\n",
    "        if type(tree) == type(\"string\"):\n",
    "            # Have reached a leaf\n",
    "            return tree\n",
    "        else:\n",
    "            a = list(tree.keys())[0]\n",
    "            for i in range(len(self.newFeatureNames)):\n",
    "                if newFeatureNames[i] == a:\n",
    "                    break\n",
    "\n",
    "            try:\n",
    "                t = tree[a][datapoint[i]]\n",
    "                return self.classify(t, datapoint)\n",
    "            except:\n",
    "                return None\n",
    "\n",
    "    def classifyAll(self, tree, data):\n",
    "        results = []\n",
    "        for i in range(len(data)):\n",
    "            results.append(self.classify(tree, data[i]))\n",
    "        return results\n",
    "\n",
    "    def make_tree(self, data, classes, featureNames, maxlevel=-1, level=0, forest=0):\n",
    "        \"\"\" The main function, which recursively constructs the tree\"\"\"\n",
    "\n",
    "        nData = len(data)\n",
    "        nFeatures = len(data[0])\n",
    "\n",
    "        try:\n",
    "            self.featureNames\n",
    "        except:\n",
    "            self.featureNames = featureNames\n",
    "\n",
    "        # List the possible classes\n",
    "        newClasses = []\n",
    "        for aclass in classes:\n",
    "            if newClasses.count(aclass) == 0:\n",
    "                newClasses.append(aclass)\n",
    "\n",
    "        # Compute the default class (and total entropy)\n",
    "        frequency = np.zeros(len(newClasses))\n",
    "\n",
    "        totalEntropy = 0\n",
    "        totalGini = 0\n",
    "        index = 0\n",
    "        for aclass in newClasses:\n",
    "            frequency[index] = classes.count(aclass)\n",
    "            totalEntropy += self.calc_entropy(float(frequency[index]) / nData)\n",
    "            totalGini += (float(frequency[index]) / nData) ** 2\n",
    "\n",
    "            index += 1\n",
    "\n",
    "        totalGini = 1 - totalGini\n",
    "        default = classes[np.argmax(frequency)]\n",
    "\n",
    "        if nData == 0 or nFeatures == 0 or (maxlevel >= 0 and level > maxlevel):\n",
    "            # Have reached an empty branch\n",
    "            return default\n",
    "        elif classes.count(classes[0]) == nData:\n",
    "            # Only 1 class remains\n",
    "            return classes[0]\n",
    "        else:\n",
    "\n",
    "            # Choose which feature is best\n",
    "            gain = np.zeros(nFeatures)\n",
    "            ggain = np.zeros(nFeatures)\n",
    "            featureSet = range(nFeatures)\n",
    "            if forest != 0:\n",
    "                np.random.shuffle(featureSet)\n",
    "                featureSet = featureSet[0:forest]\n",
    "            for feature in featureSet:\n",
    "                g, gg = self.calc_info_gain(data, classes, feature)\n",
    "                gain[feature] = totalEntropy - g\n",
    "                ggain[feature] = totalGini - gg\n",
    "\n",
    "            bestFeature = np.argmax(gain)\n",
    "            tree = {featureNames[bestFeature]: {}}\n",
    "\n",
    "            # List the values that bestFeature can take\n",
    "            values = []\n",
    "            for datapoint in data:\n",
    "                if datapoint[feature] not in values:\n",
    "                    values.append(datapoint[bestFeature])\n",
    "\n",
    "            for value in values:\n",
    "                # Find the datapoints with each feature value\n",
    "                newData = []\n",
    "                newClasses = []\n",
    "                index = 0\n",
    "                for datapoint in data:\n",
    "                    if datapoint[bestFeature] == value:\n",
    "                        if bestFeature == 0:\n",
    "                            newdatapoint = datapoint[1:]\n",
    "                            newNames = featureNames[1:]\n",
    "                        elif bestFeature == nFeatures:\n",
    "                            newdatapoint = datapoint[:-1]\n",
    "                            newNames = featureNames[:-1]\n",
    "                        else:\n",
    "                            newdatapoint = datapoint[:bestFeature]\n",
    "                            newdatapoint.extend(datapoint[bestFeature + 1:])\n",
    "                            newNames = featureNames[:bestFeature]\n",
    "                            newNames.extend(featureNames[bestFeature + 1:])\n",
    "                        newData.append(newdatapoint)\n",
    "                        newClasses.append(classes[index])\n",
    "                    index += 1\n",
    "\n",
    "                # Now recurse to the next level\n",
    "                subtree = self.make_tree(newData, newClasses, newNames, maxlevel, level + 1, forest)\n",
    "\n",
    "                # And on returning, add the subtree on to the tree\n",
    "                tree[featureNames[bestFeature]][value] = subtree\n",
    "\n",
    "            return tree\n",
    "\n",
    "    def printTree(self, tree, name):\n",
    "        if type(tree) == dict:\n",
    "            print(name, list(tree.keys())[0])\n",
    "            for item in list(list(tree.values())[0].keys()):\n",
    "                print(name, item)\n",
    "                self.printTree(list(tree.values())[0][item], name + \"\\t\")\n",
    "        else:\n",
    "            print\n",
    "            name, \"\\t->\\t\", tree\n",
    "\n",
    "    def calc_entropy(self, p):\n",
    "        if p != 0:\n",
    "            return -p * np.log2(p)\n",
    "        else:\n",
    "            return 0\n",
    "\n",
    "    def calc_info_gain(self, data, classes, feature):\n",
    "\n",
    "        # Calculates the information gain based on both entropy and the Gini impurity\n",
    "        gain = 0\n",
    "        ggain = 0\n",
    "        nData = len(data)\n",
    "\n",
    "        # List the values that feature can take\n",
    "\n",
    "        values = []\n",
    "        for datapoint in data:\n",
    "            if datapoint[feature] not in values:\n",
    "                values.append(datapoint[feature])\n",
    "\n",
    "        featureCounts = np.zeros(len(values))\n",
    "        entropy = np.zeros(len(values))\n",
    "        gini = np.zeros(len(values))\n",
    "        valueIndex = 0\n",
    "        # Find where those values appear in data[feature] and the corresponding class\n",
    "        for value in values:\n",
    "            dataIndex = 0\n",
    "            newClasses = []\n",
    "            for datapoint in data:\n",
    "                if datapoint[feature] == value:\n",
    "                    featureCounts[valueIndex] += 1\n",
    "                    newClasses.append(classes[dataIndex])\n",
    "                dataIndex += 1\n",
    "\n",
    "            # Get the values in newClasses\n",
    "            classValues = []\n",
    "            for aclass in newClasses:\n",
    "                if classValues.count(aclass) == 0:\n",
    "                    classValues.append(aclass)\n",
    "\n",
    "            classCounts = np.zeros(len(classValues))\n",
    "            classIndex = 0\n",
    "            for classValue in classValues:\n",
    "                for aclass in newClasses:\n",
    "                    if aclass == classValue:\n",
    "                        classCounts[classIndex] += 1\n",
    "                classIndex += 1\n",
    "\n",
    "            for classIndex in range(len(classValues)):\n",
    "                entropy[valueIndex] += self.calc_entropy(float(classCounts[classIndex]) / np.sum(classCounts))\n",
    "                gini[valueIndex] += (float(classCounts[classIndex]) / np.sum(classCounts)) ** 2\n",
    "\n",
    "            # Computes both the Gini gain and the entropy\n",
    "            gain = gain + float(featureCounts[valueIndex]) / nData * entropy[valueIndex]\n",
    "            ggain = ggain + float(featureCounts[valueIndex]) / nData * gini[valueIndex]\n",
    "            valueIndex += 1\n",
    "        return gain, 1 - ggain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "tree = dtree()\n",
    "data,classes,features = tree.read_data('car.data')\n",
    "# data\n",
    "# classes\n",
    "# features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1728, 6)"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.shape(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['buying', 'maintenance', 'doors', 'persons', 'luggage', 'safety']"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = data[::2][:]\n",
    "test = data[1::2][:]\n",
    "trainc = classes[::2]\n",
    "testc = classes[1::2]\n",
    "# train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# trainc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "t = tree.make_tree(train, trainc, features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class bagger:\n",
    "    \"\"\"The bagging algorithm based on the decision tree of Chapter 6\"\"\"\n",
    "\n",
    "    def __init__(self):\n",
    "        \"\"\" Constructor \"\"\"\n",
    "        self.tree = dtree()\n",
    "\n",
    "    def bag(self, data, targets, features, nSamples):\n",
    "\n",
    "        nPoints = np.shape(data)[0]\n",
    "        nDim = np.shape(data)[1]\n",
    "        self.nSamples = nSamples\n",
    "\n",
    "        # Compute bootstrap samples\n",
    "        samplePoints = np.random.randint(0, nPoints, (nPoints, nSamples))\n",
    "        classifiers = []\n",
    "\n",
    "        for i in range(nSamples):\n",
    "            sample = []\n",
    "            sampleTarget = []\n",
    "            for j in range(nPoints):\n",
    "                sample.append(data[samplePoints[j, i]])\n",
    "                sampleTarget.append(targets[samplePoints[j, i]])\n",
    "            # Train classifiers\n",
    "            classifiers.append(self.tree.make_tree(sample, sampleTarget, features, 1))\n",
    "\n",
    "        return classifiers\n",
    "\n",
    "    def bagclass(self, classifiers, data):\n",
    "\n",
    "        decision = []\n",
    "        # Majority voting\n",
    "        for j in range(len(data)):\n",
    "            outputs = []\n",
    "            # print data[j]\n",
    "            for i in range(self.nSamples):\n",
    "                out = self.tree.classify(classifiers[i], data[j])\n",
    "                \n",
    "                if out is not None:\n",
    "                    outputs.append(out)\n",
    "            # List the possible outputs\n",
    "            out = []\n",
    "            for each in outputs:\n",
    "                if out.count(each) == 0:\n",
    "                    out.append(each)\n",
    "            frequency = np.zeros(len(out))\n",
    "\n",
    "            index = 0\n",
    "            if len(out) > 0:\n",
    "                for each in out:\n",
    "                    frequency[index] = outputs.count(each)\n",
    "                    index += 1\n",
    "                decision.append(out[frequency.argmax()])\n",
    "            else:\n",
    "                decision.append(None)\n",
    "        return decision"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class random_forest:\n",
    "    \n",
    "    def __init__(self):\n",
    "        self.b = bagger()\n",
    "        \n",
    "    def classify(self, train, trainc, features, nSamples):\n",
    "        c = self.b.bag(train, trainc, features, nSamples)\n",
    "        return self.b.bagclass(c, test)\n",
    "        \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "rf = random_forest()\n",
    "out = rf.classify(train, trainc, features, 100)\n",
    "\n",
    "a = np.zeros(len(out))\n",
    " \n",
    "for i in range(len(out)):\n",
    "    if out[i] == testc[i]:\n",
    "        a[i] = 1\n",
    "print(\"-----\")\n",
    "print(\"Random forest\")\n",
    "print(\"Number correctly predicted\",str(np.sum(a)))\n",
    "print(\"Number of testpoints \",str(len(a)))\n",
    "print(\"Percentage Accuracy \",str(np.sum(a)/len(a)*100.0))\n",
    "print(out)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
